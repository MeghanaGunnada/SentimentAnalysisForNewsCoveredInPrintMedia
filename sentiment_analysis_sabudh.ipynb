{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "jSIeKXphKNhp",
        "outputId": "09b3f6e5-4b54-483a-bddc-71ce0c17adef"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting newsapi-python\n",
            "  Downloading newsapi_python-0.2.7-py2.py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.0.3)\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.10/dist-packages (3.1.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.31.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n",
            "Collecting python-docx\n",
            "  Downloading python_docx-1.1.2-py3-none-any.whl.metadata (2.0 kB)\n",
            "Collecting vaderSentiment\n",
            "  Downloading vaderSentiment-3.3.2-py2.py3-none-any.whl.metadata (572 bytes)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.25.2)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.10/dist-packages (from openpyxl) (1.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.7.4)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.5)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from python-docx) (4.9.4)\n",
            "Requirement already satisfied: typing-extensions>=4.9.0 in /usr/local/lib/python3.10/dist-packages (from python-docx) (4.12.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Downloading newsapi_python-0.2.7-py2.py3-none-any.whl (7.9 kB)\n",
            "Downloading python_docx-1.1.2-py3-none-any.whl (244 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.3/244.3 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading vaderSentiment-3.3.2-py2.py3-none-any.whl (125 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.0/126.0 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: python-docx, vaderSentiment, newsapi-python\n",
            "Successfully installed newsapi-python-0.2.7 python-docx-1.1.2 vaderSentiment-3.3.2\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fetching articles...\n",
            "Articles fetched successfully.\n",
            "Fetching article content...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-c2280cf537e8>\u001b[0m in \u001b[0;36m<cell line: 62>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;31m# Fetch article content and remove bad URLs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Fetching article content...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'full_content'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'url'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetch_article_content\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0mworking_urls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbad_requests\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_working_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'url'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[1;32m   4628\u001b[0m         \u001b[0mdtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat64\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4629\u001b[0m         \"\"\"\n\u001b[0;32m-> 4630\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mSeriesApply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4631\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4632\u001b[0m     def _reduce(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1023\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m         \u001b[0;31m# self.f is Callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1025\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1026\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1027\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1074\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1075\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1076\u001b[0;31m                 mapped = lib.map_infer(\n\u001b[0m\u001b[1;32m   1077\u001b[0m                     \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1078\u001b[0m                     \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-c2280cf537e8>\u001b[0m in \u001b[0;36mfetch_article_content\u001b[0;34m(url)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mfetch_article_content\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0msoup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'html.parser'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mparagraphs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'p'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/api.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     71\u001b[0m     \"\"\"\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"get\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/api.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    587\u001b[0m         }\n\u001b[1;32m    588\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m         \u001b[0;31m# Send the request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 703\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    704\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    705\u001b[0m         \u001b[0;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    484\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 486\u001b[0;31m             resp = conn.urlopen(\n\u001b[0m\u001b[1;32m    487\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m                 \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    789\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    790\u001b[0m             \u001b[0;31m# Make the request on the HTTPConnection object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 791\u001b[0;31m             response = self._make_request(\n\u001b[0m\u001b[1;32m    792\u001b[0m                 \u001b[0mconn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    793\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    535\u001b[0m         \u001b[0;31m# Receive the response from the server\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    536\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 537\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    538\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mBaseSSLError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_timeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mread_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/connection.py\u001b[0m in \u001b[0;36mgetresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m         \u001b[0;31m# Get the response from http.client.HTTPConnection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 461\u001b[0;31m         \u001b[0mhttplib_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    462\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    463\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/http/client.py\u001b[0m in \u001b[0;36mgetresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1373\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1374\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1375\u001b[0;31m                 \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1376\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1377\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/http/client.py\u001b[0m in \u001b[0;36mbegin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    316\u001b[0m         \u001b[0;31m# read until we get a non-100 response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 318\u001b[0;31m             \u001b[0mversion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreason\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    319\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mCONTINUE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/http/client.py\u001b[0m in \u001b[0;36m_read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 279\u001b[0;31m         \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_MAXLINE\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"iso-8859-1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    280\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0m_MAXLINE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mLineTooLong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"status line\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 705\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    706\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/ssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1301\u001b[0m                   \u001b[0;34m\"non-zero flags not allowed in calls to recv_into() on %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1302\u001b[0m                   self.__class__)\n\u001b[0;32m-> 1303\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1304\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1305\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1157\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1158\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1159\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1160\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1161\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Install necessary libraries\n",
        "!pip install newsapi-python pandas openpyxl requests beautifulsoup4 python-docx vaderSentiment\n",
        "\n",
        "from newsapi import NewsApiClient\n",
        "import pandas as pd\n",
        "import os\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from google.colab import files\n",
        "import nltk\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "import re\n",
        "\n",
        "# Download necessary nltk data\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Initialize NewsApiClient\n",
        "my_api_key = \"f57a815c8fdb43acacc42aa1f1b814d8\"\n",
        "newsapi = NewsApiClient(api_key=my_api_key)\n",
        "\n",
        "# Function to fetch article content\n",
        "def fetch_article_content(url):\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "        paragraphs = soup.find_all('p')\n",
        "        full_content = '\\n'.join([para.get_text() for para in paragraphs])\n",
        "        return full_content if full_content.strip() != '' else 'removed'\n",
        "    except Exception as e:\n",
        "        return 'removed'\n",
        "\n",
        "# Function to get working URLs\n",
        "def get_working_url(urls):\n",
        "    working_urls = []\n",
        "    bad_requests = []\n",
        "    for url in urls:\n",
        "        try:\n",
        "            response = requests.get(url)\n",
        "            if response.status_code == 200:\n",
        "                working_urls.append(url)\n",
        "            else:\n",
        "                bad_requests.append(url)\n",
        "        except Exception as e:\n",
        "            bad_requests.append(url)\n",
        "    return working_urls, bad_requests\n",
        "\n",
        "# Fetch articles\n",
        "print(\"Fetching articles...\")\n",
        "data = newsapi.get_everything(q='indian elections 2024', language='en',page_size=100)\n",
        "\n",
        "if data['status'] != 'ok':\n",
        "    raise Exception(\"Failed to fetch data from News API\")\n",
        "\n",
        "articles = data['articles']\n",
        "df = pd.DataFrame(articles)\n",
        "print(\"Articles fetched successfully.\")\n",
        "\n",
        "# Fetch article content and remove bad URLs\n",
        "print(\"Fetching article content...\")\n",
        "df['full_content'] = df['url'].apply(fetch_article_content)\n",
        "working_urls, bad_requests = get_working_url(df['url'])\n",
        "\n",
        "# Filter out rows with 'removed' content\n",
        "df_filtered = df[df['full_content'] != 'removed']\n",
        "print(f\"Filtered DataFrame has {len(df_filtered)} rows.\")\n",
        "\n",
        "# Initialize Sentiment Analyzer\n",
        "analyzer = SentimentIntensityAnalyzer()\n",
        "\n",
        "# List of political parties to identify in the articles, including common variations\n",
        "party_keywords = {\n",
        "    'BJP': ['BJP', 'Bharatiya Janata Party'],\n",
        "    'INC': ['Congress', 'Indian National Congress', 'INC'],\n",
        "    'AAP': ['AAP', 'Aam Aadmi Party'],\n",
        "    'CPI': ['CPI', 'Communist Party of India'],\n",
        "    'CPM': ['CPM', 'Communist Party of India (Marxist)', 'CPI(M)'],\n",
        "    'NCP': ['NCP', 'Nationalist Congress Party'],\n",
        "    'BSP': ['BSP', 'Bahujan Samaj Party'],\n",
        "    'SP': ['SP', 'Samajwadi Party'],\n",
        "    'RJD': ['RJD', 'Rashtriya Janata Dal'],\n",
        "    'JD(U)': ['JD(U)', 'Janata Dal (United)'],\n",
        "    'TMC': ['TMC', 'All India Trinamool Congress', 'Trinamool Congress'],\n",
        "    'AIADMK': ['AIADMK', 'All India Anna Dravida Munnetra Kazhagam'],\n",
        "    'DMK': ['DMK', 'Dravida Munnetra Kazhagam'],\n",
        "    'Shiv Sena': ['Shiv Sena', 'Shiv Sena'],\n",
        "    'TRS': ['TRS', 'Telangana Rashtra Samithi'],\n",
        "    'YSRCP': ['YSRCP', 'Yuvajana Sramika Rythu Congress Party'],\n",
        "    'TDP': ['TDP', 'Telugu Desam Party'],\n",
        "    'LJP': ['LJP', 'Lok Janshakti Party'],\n",
        "    'RLD': ['RLD', 'Rashtriya Lok Dal'],\n",
        "    'AIMIM': ['AIMIM', 'All India Majlis-e-Ittehadul Muslimeen'],\n",
        "    'JD(S)': ['JD(S)', 'Janata Dal (Secular)'],\n",
        "    'INLD': ['INLD', 'Indian National Lok Dal'],\n",
        "    'JMM': ['JMM', 'Jharkhand Mukti Morcha'],\n",
        "    'SAD': ['SAD', 'Shiromani Akali Dal'],\n",
        "    'RSP': ['RSP', 'Revolutionary Socialist Party'],\n",
        "    'AGP': ['AGP', 'Asom Gana Parishad'],\n",
        "    'BPF': ['BPF', 'Bodoland People\\'s Front'],\n",
        "    'SDF': ['SDF', 'Sikkim Democratic Front'],\n",
        "    'MNDF': ['MNDF', 'Mizo National Front'],\n",
        "    'UDP': ['UDP', 'United Democratic Party (Meghalaya)'],\n",
        "    'NPF': ['NPF', 'Naga People\\'s Front'],\n",
        "    'ZPM': ['ZPM', 'Zoram People\\'s Movement'],\n",
        "    'KC(M)': ['KC(M)', 'Kerala Congress (M)'],\n",
        "    'PDP': ['PDP', 'Peoples Democratic Party'],\n",
        "    'NC': ['NC', 'National Conference']\n",
        "\n",
        "}\n",
        "\n",
        "# Apply sentiment analysis\n",
        "def analyze_sentiment(text):\n",
        "    scores = analyzer.polarity_scores(text)\n",
        "    return scores['compound']\n",
        "\n",
        "def get_sentiment_label(score):\n",
        "    if score >= 0.5:\n",
        "        return 'Positive'\n",
        "    elif score <= -0.5:\n",
        "        return 'Negative'\n",
        "    else:\n",
        "        return 'Neutral'\n",
        "\n",
        "import spacy\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "import pandas as pd\n",
        "\n",
        "# Load spaCy model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Function to extract party names from text\n",
        "def extract_parties(text):\n",
        "    doc = nlp(text)\n",
        "    entities = [ent.text for ent in doc.ents if ent.label_ == \"ORG\"]\n",
        "    parties_in_text = []\n",
        "    for party, keywords in party_keywords.items():\n",
        "        if any(keyword in entities for keyword in keywords):\n",
        "            parties_in_text.append(party)\n",
        "    return parties_in_text\n",
        "\n",
        "# Function to get sentiment of a text\n",
        "analyzer = SentimentIntensityAnalyzer()\n",
        "\n",
        "def get_sentiment(text):\n",
        "    sentiment = analyzer.polarity_scores(text)\n",
        "    return sentiment['compound']\n",
        "\n",
        "# Function to determine the main party based on weighted sentiment\n",
        "def determine_main_party_weighted(text):\n",
        "    doc = nlp(text)\n",
        "    sentences = list(doc.sents)\n",
        "\n",
        "    party_sentiments = {party: 0 for party in party_keywords}\n",
        "    party_counts = {party: 0 for party in party_keywords}\n",
        "\n",
        "    for sentence in sentences:\n",
        "        sentence_text = sentence.text\n",
        "        sentiment = get_sentiment(sentence_text)\n",
        "        parties = extract_parties(sentence_text)\n",
        "\n",
        "        for party in parties:\n",
        "            party_sentiments[party] += sentiment\n",
        "            party_counts[party] += 1\n",
        "\n",
        "    weighted_scores = {party: party_sentiments[party] / party_counts[party] if party_counts[party] > 0 else 0 for party in party_keywords}\n",
        "    max_party_by_weighted_score = max(weighted_scores, key=weighted_scores.get)\n",
        "    max_party_by_count = max(party_counts, key=party_counts.get)\n",
        "\n",
        "    if weighted_scores[max_party_by_weighted_score] != 0:\n",
        "        return max_party_by_weighted_score\n",
        "    else:\n",
        "        return max_party_by_count if party_counts[max_party_by_count] != 0 else 'Unknown'\n",
        "\n",
        "\n",
        "\n",
        "# Ensure all rows are subjected to sentiment analysis\n",
        "df_filtered['sentiment_score'] = df_filtered['full_content'].apply(analyze_sentiment)\n",
        "df_filtered['party'] = df_filtered['full_content'].apply(determine_main_party_weighted)\n",
        "df_filtered['sentiment_label'] = df_filtered['sentiment_score'].apply(get_sentiment_label)\n",
        "\n",
        "# Filter out rows where the party is 'Unknown'\n",
        "df_final = df_filtered[df_filtered['party'] != 'Unknown']\n",
        "\n",
        "# Check the number of rows processed\n",
        "processed_rows = len(df_final)\n",
        "total_rows = len(df_filtered)\n",
        "print(f\"Sentiment analysis completed for {processed_rows} out of {total_rows} rows with identified parties.\")\n",
        "\n",
        "# Save filtered DataFrame to Excel\n",
        "file_path = '/content/output.xlsx'\n",
        "df_final.to_excel(file_path, sheet_name='Sheet1', index=False)\n",
        "\n",
        "if os.path.exists(file_path):\n",
        "    print(\"Excel file found. Attempting to download...\")\n",
        "    files.download(file_path)\n",
        "else:\n",
        "    print(\"Excel file not found.\")\n",
        "\n",
        "# Save filtered DataFrame to CSV\n",
        "csv_file_path = '/content/output_full_content.csv'\n",
        "df_final.to_csv(csv_file_path, index=False)\n",
        "\n",
        "# Verify the content of the saved CSV\n",
        "if os.path.exists(csv_file_path):\n",
        "    print(\"CSV file with full content found.\")\n",
        "    df_check = pd.read_csv(csv_file_path)\n",
        "    print(\"Preview of saved CSV:\")\n",
        "    print(df_check.head())  # Display the first few rows for verification\n",
        "    files.download(csv_file_path)\n",
        "else:\n",
        "    print(\"CSV file not found.\")\n",
        "\n",
        "# Save bad request URLs to Excel\n",
        "bad_requests_df = pd.DataFrame({'Bad URLs': bad_requests})\n",
        "bad_requests_file_path = '/content/badrequests.xlsx'\n",
        "bad_requests_df.to_excel(bad_requests_file_path, index=False)\n",
        "\n",
        "if os.path.exists(bad_requests_file_path):\n",
        "    print(\"Bad request URLs saved to 'badrequests.xlsx'\")\n",
        "else:\n",
        "    print(\"Failed to save bad request URLs.\")\n",
        "\n",
        "print(\"Script executed successfully\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "08mc1W21Npbv",
        "outputId": "3230a43a-7479-4f44-8db6-15d3577269c2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: newsapi-python in /usr/local/lib/python3.10/dist-packages (0.2.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.0.3)\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.10/dist-packages (3.1.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.31.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n",
            "Requirement already satisfied: python-docx in /usr/local/lib/python3.10/dist-packages (1.1.2)\n",
            "Requirement already satisfied: vaderSentiment in /usr/local/lib/python3.10/dist-packages (3.3.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.25.2)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.10/dist-packages (from openpyxl) (1.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.6.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.5)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from python-docx) (4.9.4)\n",
            "Requirement already satisfied: typing-extensions>=4.9.0 in /usr/local/lib/python3.10/dist-packages (from python-docx) (4.12.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fetching articles...\n",
            "Articles fetched successfully.\n",
            "Fetching article content...\n",
            "Filtered DataFrame has 97 rows.\n",
            "                                        article_text party  sentiment\n",
            "0  The BJP has launched its new campaign for the ...  None    -0.2263\n",
            "1   AAP's manifesto focuses on health and education.  None     0.0000\n",
            "Sentiment analysis completed for 72 out of 97 rows with identified parties.\n",
            "Excel file found. Attempting to download...\n"
          ]
        },
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_acfa6490-f266-40ab-989d-e7b7f4c6b264\", \"output_new.xlsx\", 128307)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CSV file with full content found.\n",
            "Preview of saved CSV:\n",
            "                                              source            author  \\\n",
            "0  {'id': 'al-jazeera-english', 'name': 'Al Jazee...            AJLabs   \n",
            "1  {'id': 'al-jazeera-english', 'name': 'Al Jazee...  Al Jazeera Staff   \n",
            "2  {'id': 'al-jazeera-english', 'name': 'Al Jazee...       Saif Khalid   \n",
            "3  {'id': 'al-jazeera-english', 'name': 'Al Jazee...  Al Jazeera Staff   \n",
            "4  {'id': 'al-jazeera-english', 'name': 'Al Jazee...  Al Jazeera Staff   \n",
            "\n",
            "                                               title  \\\n",
            "0     Mapping the results of the India election 2024   \n",
            "1  India election results: Big wins, losses and s...   \n",
            "2  India election results: Did ‘secular’ parties ...   \n",
            "3  India Lok Sabha election 2024 Phase 7: Who vot...   \n",
            "4  India Lok Sabha election 2024 Phase 6: Who vot...   \n",
            "\n",
            "                                         description  \\\n",
            "0  The Bharatiya Janata Party, together with its ...   \n",
            "1  A tight Varanasi race and BJP's Maharashtra do...   \n",
            "2  Scared of the BJP describing them as pro-Musli...   \n",
            "3  Voters will decide the fate of 904 candidates ...   \n",
            "4  Voters from eight states and union territories...   \n",
            "\n",
            "                                                 url  \\\n",
            "0  https://www.aljazeera.com/news/2024/6/6/mappin...   \n",
            "1  https://www.aljazeera.com/news/2024/6/4/india-...   \n",
            "2  https://www.aljazeera.com/news/2024/6/5/india-...   \n",
            "3  https://www.aljazeera.com/news/2024/5/31/india...   \n",
            "4  https://www.aljazeera.com/news/2024/5/24/india...   \n",
            "\n",
            "                                          urlToImage           publishedAt  \\\n",
            "0  https://www.aljazeera.com/wp-content/uploads/2...  2024-06-06T04:31:45Z   \n",
            "1  https://www.aljazeera.com/wp-content/uploads/2...  2024-06-04T11:49:26Z   \n",
            "2  https://www.aljazeera.com/wp-content/uploads/2...  2024-06-05T12:04:02Z   \n",
            "3  https://www.aljazeera.com/wp-content/uploads/2...  2024-05-31T07:41:15Z   \n",
            "4  https://www.aljazeera.com/wp-content/uploads/2...  2024-05-24T07:13:20Z   \n",
            "\n",
            "                                             content  \\\n",
            "0  With all of Indias 640 million votes counted f...   \n",
            "1  As Indias election results become clearer, wit...   \n",
            "2  As Indian opposition leader Rahul Gandhi addre...   \n",
            "3  Indians will cast ballots in the last phase of...   \n",
            "4  Indias staggered general election is heading t...   \n",
            "\n",
            "                                        full_content  sentiment_score  \\\n",
            "0  The Bharatiya Janata Party has fallen short of...           0.9982   \n",
            "1  A tight Varanasi race and BJP’s Maharashtra do...           0.9995   \n",
            "2  Scared of the BJP describing them as pro-Musli...           0.9985   \n",
            "3  Voters will decide the fate of 904 candidates ...           0.9951   \n",
            "4  Voters from eight states and union territories...           0.9961   \n",
            "\n",
            "  sentiment_label party  \n",
            "0        Positive   BJP  \n",
            "1        Positive   BJP  \n",
            "2        Positive   BJP  \n",
            "3        Positive   BJP  \n",
            "4        Positive   BJP  \n"
          ]
        },
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_6701b765-e907-461b-b4e7-56f09ae29037\", \"output_new_full_content.csv\", 374686)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Bad request URLs saved to 'badrequests_new.xlsx'\n",
            "Script executed successfully\n"
          ]
        }
      ],
      "source": [
        "# Install necessary libraries\n",
        "!pip install newsapi-python pandas openpyxl requests beautifulsoup4 python-docx vaderSentiment\n",
        "\n",
        "from newsapi import NewsApiClient\n",
        "import pandas as pd\n",
        "import os\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from google.colab import files\n",
        "import nltk\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "import re\n",
        "\n",
        "# Download necessary nltk data\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Initialize NewsApiClient\n",
        "my_api_key = \"f57a815c8fdb43acacc42aa1f1b814d8\"\n",
        "newsapi = NewsApiClient(api_key=my_api_key)\n",
        "\n",
        "# Function to fetch article content\n",
        "def fetch_article_content(url):\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "        paragraphs = soup.find_all('p')\n",
        "        full_content = '\\n'.join([para.get_text() for para in paragraphs])\n",
        "        return full_content if full_content.strip() != '' else 'removed'\n",
        "    except Exception as e:\n",
        "        return 'removed'\n",
        "\n",
        "# Function to get working URLs\n",
        "def get_working_url(urls):\n",
        "    working_urls = []\n",
        "    bad_requests = []\n",
        "    for url in urls:\n",
        "        try:\n",
        "            response = requests.get(url)\n",
        "            if response.status_code == 200:\n",
        "                working_urls.append(url)\n",
        "            else:\n",
        "                bad_requests.append(url)\n",
        "        except Exception as e:\n",
        "            bad_requests.append(url)\n",
        "    return working_urls, bad_requests\n",
        "\n",
        "# Fetch articles with a different query\n",
        "print(\"Fetching articles...\")\n",
        "data = newsapi.get_everything(q='Samajwadi Party OR SP OR Bahujan Samaj Party OR BSP OR Communist Party of India OR CPI', language='en', page_size=100)\n",
        "\n",
        "if data['status'] != 'ok':\n",
        "    raise Exception(\"Failed to fetch data from News API\")\n",
        "\n",
        "articles = data['articles']\n",
        "df = pd.DataFrame(articles)\n",
        "print(\"Articles fetched successfully.\")\n",
        "\n",
        "# Fetch article content and remove bad URLs\n",
        "print(\"Fetching article content...\")\n",
        "df['full_content'] = df['url'].apply(fetch_article_content)\n",
        "working_urls, bad_requests = get_working_url(df['url'])\n",
        "\n",
        "# Filter out rows with 'removed' content\n",
        "df_filtered = df[df['full_content'] != 'removed']\n",
        "print(f\"Filtered DataFrame has {len(df_filtered)} rows.\")\n",
        "\n",
        "# Initialize Sentiment Analyzer\n",
        "analyzer = SentimentIntensityAnalyzer()\n",
        "\n",
        "# List of political parties to identify in the articles, including common variations\n",
        "party_keywords = {\n",
        "      'BJP': ['BJP', 'Bharatiya Janata Party'],\n",
        "    'INC': ['Congress', 'Indian National Congress', 'INC'],\n",
        "    'AAP': ['AAP', 'Aam Aadmi Party'],\n",
        "    'CPI': ['CPI', 'Communist Party of India'],\n",
        "    'CPM': ['CPM', 'Communist Party of India (Marxist)', 'CPI(M)'],\n",
        "    'NCP': ['NCP', 'Nationalist Congress Party'],\n",
        "    'BSP': ['BSP', 'Bahujan Samaj Party'],\n",
        "    'SP': ['SP', 'Samajwadi Party'],\n",
        "    'RJD': ['RJD', 'Rashtriya Janata Dal'],\n",
        "    'JD(U)': ['JD(U)', 'Janata Dal (United)'],\n",
        "    'TMC': ['TMC', 'All India Trinamool Congress', 'Trinamool Congress'],\n",
        "    'AIADMK': ['AIADMK', 'All India Anna Dravida Munnetra Kazhagam'],\n",
        "    'DMK': ['DMK', 'Dravida Munnetra Kazhagam'],\n",
        "    'Shiv Sena': ['Shiv Sena', 'Shiv Sena'],\n",
        "    'TRS': ['TRS', 'Telangana Rashtra Samithi'],\n",
        "    'YSRCP': ['YSRCP', 'Yuvajana Sramika Rythu Congress Party'],\n",
        "    'TDP': ['TDP', 'Telugu Desam Party'],\n",
        "    'LJP': ['LJP', 'Lok Janshakti Party'],\n",
        "    'RLD': ['RLD', 'Rashtriya Lok Dal'],\n",
        "    'AIMIM': ['AIMIM', 'All India Majlis-e-Ittehadul Muslimeen'],\n",
        "    'JD(S)': ['JD(S)', 'Janata Dal (Secular)'],\n",
        "    'INLD': ['INLD', 'Indian National Lok Dal'],\n",
        "    'JMM': ['JMM', 'Jharkhand Mukti Morcha'],\n",
        "    'SAD': ['SAD', 'Shiromani Akali Dal'],\n",
        "    'RSP': ['RSP', 'Revolutionary Socialist Party'],\n",
        "    'AGP': ['AGP', 'Asom Gana Parishad'],\n",
        "    'BPF': ['BPF', 'Bodoland People\\'s Front'],\n",
        "    'SDF': ['SDF', 'Sikkim Democratic Front'],\n",
        "    'MNDF': ['MNDF', 'Mizo National Front'],\n",
        "    'UDP': ['UDP', 'United Democratic Party (Meghalaya)'],\n",
        "    'NPF': ['NPF', 'Naga People\\'s Front'],\n",
        "    'ZPM': ['ZPM', 'Zoram People\\'s Movement'],\n",
        "    'KC(M)': ['KC(M)', 'Kerala Congress (M)'],\n",
        "    'PDP': ['PDP', 'Peoples Democratic Party'],\n",
        "    'NC': ['NC', 'National Conference']\n",
        "}\n",
        "\n",
        "# Apply sentiment analysis\n",
        "def analyze_sentiment(text):\n",
        "    scores = analyzer.polarity_scores(text)\n",
        "    return scores['compound']\n",
        "\n",
        "def get_sentiment_label(score):\n",
        "    if score >= 0.05:\n",
        "        return 'Positive'\n",
        "    elif score <= -0.05:\n",
        "        return 'Negative'\n",
        "    else:\n",
        "        return 'Neutral'\n",
        "\n",
        "import spacy\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "import pandas as pd\n",
        "\n",
        "# Load spaCy model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Function to extract party names from text\n",
        "def extract_parties(text):\n",
        "    doc = nlp(text)\n",
        "    entities = [ent.text for ent in doc.ents if ent.label_ == \"ORG\"]\n",
        "    parties_in_text = []\n",
        "    for party, keywords in party_keywords.items():\n",
        "        if any(keyword in entities for keyword in keywords):\n",
        "            parties_in_text.append(party)\n",
        "    return parties_in_text\n",
        "\n",
        "# Function to get sentiment of a text\n",
        "analyzer = SentimentIntensityAnalyzer()\n",
        "\n",
        "def get_sentiment(text):\n",
        "    sentiment = analyzer.polarity_scores(text)\n",
        "    return sentiment['compound']\n",
        "\n",
        "# Function to determine the main party based on weighted sentiment\n",
        "def determine_main_party_weighted(text):\n",
        "    doc = nlp(text)\n",
        "    sentences = list(doc.sents)\n",
        "\n",
        "    party_sentiments = {party: 0 for party in party_keywords}\n",
        "    party_counts = {party: 0 for party in party_keywords}\n",
        "\n",
        "    for sentence in sentences:\n",
        "        sentence_text = sentence.text\n",
        "        sentiment = get_sentiment(sentence_text)\n",
        "        parties = extract_parties(sentence_text)\n",
        "\n",
        "        for party in parties:\n",
        "            party_sentiments[party] += sentiment\n",
        "            party_counts[party] += 1\n",
        "\n",
        "    weighted_scores = {party: party_sentiments[party] / party_counts[party] if party_counts[party] > 0 else 0 for party in party_keywords}\n",
        "    max_party_by_weighted_score = max(weighted_scores, key=weighted_scores.get)\n",
        "    max_party_by_count = max(party_counts, key=party_counts.get)\n",
        "\n",
        "    if weighted_scores[max_party_by_weighted_score] != 0:\n",
        "        return max_party_by_weighted_score\n",
        "    else:\n",
        "        return max_party_by_count if party_counts[max_party_by_count] != 0 else 'Unknown'\n",
        "\n",
        "\n",
        "\n",
        "# Ensure all rows are subjected to sentiment analysis\n",
        "df_filtered['sentiment_score'] = df_filtered['full_content'].apply(analyze_sentiment)\n",
        "df_filtered['party'] = df_filtered['full_content'].apply(determine_main_party_weighted)\n",
        "df_filtered['sentiment_label'] = df_filtered['sentiment_score'].apply(get_sentiment_label)\n",
        "\n",
        "# Filter out rows where the party is 'Unknown'\n",
        "df_final = df_filtered[df_filtered['party'] != 'Unknown']\n",
        "\n",
        "# Check the number of rows processed\n",
        "processed_rows = len(df_final)\n",
        "total_rows = len(df_filtered)\n",
        "print(f\"Sentiment analysis completed for {processed_rows} out of {total_rows} rows with identified parties.\")\n",
        "\n",
        "# Save filtered DataFrame to Excel\n",
        "file_path = '/content/output_new.xlsx'\n",
        "df_final.to_excel(file_path, sheet_name='Sheet1', index=False)\n",
        "\n",
        "if os.path.exists(file_path):\n",
        "    print(\"Excel file found. Attempting to download...\")\n",
        "    files.download(file_path)\n",
        "else:\n",
        "    print(\"Excel file not found.\")\n",
        "\n",
        "# Save filtered DataFrame to CSV\n",
        "csv_file_path = '/content/output_new_full_content.csv'\n",
        "df_final.to_csv(csv_file_path, index=False)\n",
        "\n",
        "# Verify the content of the saved CSV\n",
        "if os.path.exists(csv_file_path):\n",
        "    print(\"CSV file with full content found.\")\n",
        "    df_check = pd.read_csv(csv_file_path)\n",
        "    print(\"Preview of saved CSV:\")\n",
        "    print(df_check.head())  # Display the first few rows for verification\n",
        "    files.download(csv_file_path)\n",
        "else:\n",
        "    print(\"CSV file not found.\")\n",
        "\n",
        "# Save bad request URLs to Excel\n",
        "bad_requests_df = pd.DataFrame({'Bad URLs': bad_requests})\n",
        "bad_requests_file_path = '/content/badrequests_new.xlsx'\n",
        "bad_requests_df.to_excel(bad_requests_file_path, index=False)\n",
        "\n",
        "if os.path.exists(bad_requests_file_path):\n",
        "    print(\"Bad request URLs saved to 'badrequests_new.xlsx'\")\n",
        "else:\n",
        "    print(\"Failed to save bad request URLs.\")\n",
        "\n",
        "print(\"Script executed successfully\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "qsu6v-OV6OYX",
        "outputId": "76b28e05-152b-4a32-b35a-f1f7130025eb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: newsapi-python in /usr/local/lib/python3.10/dist-packages (0.2.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.0.3)\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.10/dist-packages (3.1.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.31.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n",
            "Requirement already satisfied: python-docx in /usr/local/lib/python3.10/dist-packages (1.1.2)\n",
            "Requirement already satisfied: vaderSentiment in /usr/local/lib/python3.10/dist-packages (3.3.2)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.7.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.25.2)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.10/dist-packages (from openpyxl) (1.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.6.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.5)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from python-docx) (4.9.4)\n",
            "Requirement already satisfied: typing-extensions>=4.9.0 in /usr/local/lib/python3.10/dist-packages (from python-docx) (4.12.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.2.3)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.3.4)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.9.4)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.66.4)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.7.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (24.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.4.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.18.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.5)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.7)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.4.0,>=0.1.0->spacy) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (2.1.5)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.1.1)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fetching articles...\n",
            "Articles fetched successfully.\n",
            "Fetching article content...\n",
            "Filtered DataFrame has 85 rows.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-3-65b2ddd924e7>:155: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df_filtered['sentiment_score'] = df_filtered['full_content'].apply(analyze_sentiment)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sentiment analysis completed for 58 out of 85 rows with identified parties.\n",
            "Excel file found. Attempting to download...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-3-65b2ddd924e7>:156: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df_filtered['party'] = df_filtered['full_content'].apply(determine_main_party_weighted)\n",
            "<ipython-input-3-65b2ddd924e7>:157: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df_filtered['sentiment_label'] = df_filtered['sentiment_score'].apply(get_sentiment_label)\n"
          ]
        },
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_f57a071e-a205-4afd-ba8a-a5a713bb12c6\", \"output.xlsx\", 226064)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CSV file with full content found.\n",
            "Preview of saved CSV:\n",
            "                                              source  \\\n",
            "0  {'id': 'business-insider', 'name': 'Business I...   \n",
            "1                     {'id': 'time', 'name': 'Time'}   \n",
            "2  {'id': 'business-insider', 'name': 'Business I...   \n",
            "3  {'id': 'business-insider', 'name': 'Business I...   \n",
            "4        {'id': None, 'name': 'Yahoo Entertainment'}   \n",
            "\n",
            "                                              author  \\\n",
            "0                                     Rebecca Rommen   \n",
            "1                                    Astha Rajvanshi   \n",
            "2                                        Matthew Loh   \n",
            "3                                         Tom Porter   \n",
            "4  Vandinika Shukla, Harvard Kennedy School and B...   \n",
            "\n",
            "                                               title  \\\n",
            "0  Indian authorities seize over $1 billion worth...   \n",
            "1  The Controversy Over a New Population Study Fr...   \n",
            "2  It's been 3 days since Modi won, and we're alr...   \n",
            "3  The competition between India and China is abo...   \n",
            "4  Indian election was awash in deepfakes – but A...   \n",
            "\n",
            "                                         description  \\\n",
            "0  The winner of India's general election, which ...   \n",
            "1  Critics say the study could sow communal disco...   \n",
            "2  Modi's next term as prime minister is set to l...   \n",
            "3  India and China are locked in a battle between...   \n",
            "4  Campaigns used deepfakes to connect with voter...   \n",
            "\n",
            "                                                 url  \\\n",
            "0  https://www.businessinsider.com/indian-authori...   \n",
            "1  https://time.com/6976854/india-population-stud...   \n",
            "2  https://www.businessinsider.com/narendra-modi-...   \n",
            "3  https://www.businessinsider.com/india-china-co...   \n",
            "4  https://www.yahoo.com/news/indian-election-awa...   \n",
            "\n",
            "                                          urlToImage           publishedAt  \\\n",
            "0  https://i.insider.com/6649a39d20abc1efe8fb6473...  2024-05-19T13:22:38Z   \n",
            "1  https://api.time.com/wp-content/uploads/2024/0...  2024-05-10T15:16:47Z   \n",
            "2  https://i.insider.com/666286961cd3b17790443e58...  2024-06-07T05:59:02Z   \n",
            "3  https://i.insider.com/66631e09cc442a2f676f130d...  2024-06-08T10:03:17Z   \n",
            "4  https://s.yimg.com/ny/api/res/1.2/JhjdV20_cCeY...  2024-06-10T13:43:43Z   \n",
            "\n",
            "                                             content  \\\n",
            "0  Election duty staff cast their votes on May 13...   \n",
            "1  A working paper from an independent body that ...   \n",
            "2  India's Prime Minister Narendra Modi addresses...   \n",
            "3  Jack Taylor/Getty Images; Carlos Barria/Pool/A...   \n",
            "4  As India concluded the worlds largest election...   \n",
            "\n",
            "                                        full_content  sentiment_score party  \\\n",
            "0  The Election Commission of India (ECI) said it...           0.9132   BJP   \n",
            "1  A working paper from an independent body that ...           0.8999   BJP   \n",
            "2  As Indian Prime Minister Narendra Modi assembl...           0.9829   BJP   \n",
            "3  Narendra Modi's strongman ambitions suffered a...           0.9950   BJP   \n",
            "4  As India concluded the world’s largest electio...           0.9995   BJP   \n",
            "\n",
            "  sentiment_label  \n",
            "0        Positive  \n",
            "1        Positive  \n",
            "2        Positive  \n",
            "3        Positive  \n",
            "4        Positive  \n"
          ]
        },
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_0b9e2c8d-65a2-4f74-98bb-41e606b9a19d\", \"output_full_content.csv\", 634227)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Bad request URLs saved to 'badrequests.xlsx'\n",
            "Script executed successfully\n"
          ]
        }
      ],
      "source": [
        "!pip install newsapi-python pandas openpyxl requests beautifulsoup4 python-docx vaderSentiment spacy\n",
        "\n",
        "from newsapi import NewsApiClient\n",
        "import pandas as pd\n",
        "import os\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from google.colab import files\n",
        "import nltk\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "import spacy\n",
        "\n",
        "# Download necessary nltk data\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Load spaCy model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Initialize NewsApiClient\n",
        "my_api_key = \"f57a815c8fdb43acacc42aa1f1b814d8\"\n",
        "newsapi = NewsApiClient(api_key=my_api_key)\n",
        "\n",
        "# Function to fetch article content\n",
        "def fetch_article_content(url):\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "        paragraphs = soup.find_all('p')\n",
        "        full_content = '\\n'.join([para.get_text() for para in paragraphs])\n",
        "        return full_content if full_content.strip() != '' else 'removed'\n",
        "    except Exception as e:\n",
        "        return 'removed'\n",
        "\n",
        "# Function to get working URLs\n",
        "def get_working_url(urls):\n",
        "    working_urls = []\n",
        "    bad_requests = []\n",
        "    for url in urls:\n",
        "        try:\n",
        "            response = requests.get(url)\n",
        "            if response.status_code == 200:\n",
        "                working_urls.append(url)\n",
        "            else:\n",
        "                bad_requests.append(url)\n",
        "        except Exception as e:\n",
        "            bad_requests.append(url)\n",
        "    return working_urls, bad_requests\n",
        "\n",
        "# Fetch articles\n",
        "print(\"Fetching articles...\")\n",
        "data = newsapi.get_everything(q='indian elections 2024', language='en')\n",
        "\n",
        "if data['status'] != 'ok':\n",
        "    raise Exception(\"Failed to fetch data from News API\")\n",
        "\n",
        "articles = data['articles']\n",
        "df = pd.DataFrame(articles)\n",
        "print(\"Articles fetched successfully.\")\n",
        "\n",
        "# Fetch article content and remove bad URLs\n",
        "print(\"Fetching article content...\")\n",
        "df['full_content'] = df['url'].apply(fetch_article_content)\n",
        "working_urls, bad_requests = get_working_url(df['url'])\n",
        "\n",
        "# Filter out rows with 'removed' content\n",
        "df_filtered = df[df['full_content'] != 'removed']\n",
        "print(f\"Filtered DataFrame has {len(df_filtered)} rows.\")\n",
        "\n",
        "# Initialize Sentiment Analyzer\n",
        "analyzer = SentimentIntensityAnalyzer()\n",
        "\n",
        "# List of political parties to identify in the articles, including common variations\n",
        "party_keywords = {\n",
        "    'BJP': ['BJP', 'Bharatiya Janata Party'],\n",
        "    'INC': ['Congress', 'Indian National Congress', 'INC'],\n",
        "    'AAP': ['AAP', 'Aam Aadmi Party'],\n",
        "    'CPI': ['CPI', 'Communist Party of India'],\n",
        "    'CPM': ['CPM', 'Communist Party of India (Marxist)', 'CPI(M)'],\n",
        "    'NCP': ['NCP', 'Nationalist Congress Party'],\n",
        "    'BSP': ['BSP', 'Bahujan Samaj Party'],\n",
        "    'SP': ['SP', 'Samajwadi Party'],\n",
        "    'RJD': ['RJD', 'Rashtriya Janata Dal'],\n",
        "    'JD(U)': ['JD(U)', 'Janata Dal (United)'],\n",
        "    'TMC': ['TMC', 'All India Trinamool Congress', 'Trinamool Congress'],\n",
        "    'AIADMK': ['AIADMK', 'All India Anna Dravida Munnetra Kazhagam'],\n",
        "    'DMK': ['DMK', 'Dravida Munnetra Kazhagam'],\n",
        "    'Shiv Sena': ['Shiv Sena', 'Shiv Sena'],\n",
        "    'TRS': ['TRS', 'Telangana Rashtra Samithi'],\n",
        "    'YSRCP': ['YSRCP', 'Yuvajana Sramika Rythu Congress Party'],\n",
        "    'TDP': ['TDP', 'Telugu Desam Party'],\n",
        "    'LJP': ['LJP', 'Lok Janshakti Party'],\n",
        "    'RLD': ['RLD', 'Rashtriya Lok Dal'],\n",
        "    'AIMIM': ['AIMIM', 'All India Majlis-e-Ittehadul Muslimeen'],\n",
        "    'JD(S)': ['JD(S)', 'Janata Dal (Secular)'],\n",
        "    'INLD': ['INLD', 'Indian National Lok Dal'],\n",
        "    'JMM': ['JMM', 'Jharkhand Mukti Morcha'],\n",
        "    'SAD': ['SAD', 'Shiromani Akali Dal'],\n",
        "    'RSP': ['RSP', 'Revolutionary Socialist Party'],\n",
        "    'AGP': ['AGP', 'Asom Gana Parishad'],\n",
        "    'BPF': ['BPF', 'Bodoland People\\'s Front'],\n",
        "    'SDF': ['SDF', 'Sikkim Democratic Front'],\n",
        "    'MNDF': ['MNDF', 'Mizo National Front'],\n",
        "    'UDP': ['UDP', 'United Democratic Party (Meghalaya)'],\n",
        "    'NPF': ['NPF', 'Naga People\\'s Front'],\n",
        "    'ZPM': ['ZPM', 'Zoram People\\'s Movement'],\n",
        "    'KC(M)': ['KC(M)', 'Kerala Congress (M)'],\n",
        "    'PDP': ['PDP', 'Peoples Democratic Party'],\n",
        "    'NC': ['NC', 'National Conference']\n",
        "}\n",
        "\n",
        "# Function to extract party names from text\n",
        "def extract_parties(text):\n",
        "    doc = nlp(text)\n",
        "    entities = [ent.text for ent in doc.ents if ent.label_ == \"ORG\"]\n",
        "    parties_in_text = []\n",
        "    for party, keywords in party_keywords.items():\n",
        "        if any(keyword in entities for keyword in keywords):\n",
        "            parties_in_text.append(party)\n",
        "    return parties_in_text\n",
        "\n",
        "# Function to get sentiment of a text\n",
        "def get_sentiment(text):\n",
        "    sentiment = analyzer.polarity_scores(text)\n",
        "    return sentiment['compound']\n",
        "\n",
        "# Function to determine the main party based on weighted sentiment\n",
        "def determine_main_party_weighted(text):\n",
        "    doc = nlp(text)\n",
        "    sentences = list(doc.sents)\n",
        "\n",
        "    party_sentiments = {party: 0 for party in party_keywords}\n",
        "    party_counts = {party: 0 for party in party_keywords}\n",
        "\n",
        "    for sentence in sentences:\n",
        "        sentence_text = sentence.text\n",
        "        sentiment = get_sentiment(sentence_text)\n",
        "        parties = extract_parties(sentence_text)\n",
        "\n",
        "        for party in parties:\n",
        "            party_sentiments[party] += sentiment\n",
        "            party_counts[party] += 1\n",
        "\n",
        "    weighted_scores = {party: party_sentiments[party] / party_counts[party] if party_counts[party] > 0 else 0 for party in party_keywords}\n",
        "    max_party_by_weighted_score = max(weighted_scores, key=weighted_scores.get)\n",
        "    max_party_by_count = max(party_counts, key=party_counts.get)\n",
        "\n",
        "    if weighted_scores[max_party_by_weighted_score] != 0:\n",
        "        return max_party_by_weighted_score\n",
        "    else:\n",
        "        return max_party_by_count if party_counts[max_party_by_count] != 0 else 'Unknown'\n",
        "\n",
        "# Apply sentiment analysis\n",
        "df_filtered['sentiment_score'] = df_filtered['full_content'].apply(analyze_sentiment)\n",
        "df_filtered['party'] = df_filtered['full_content'].apply(determine_main_party_weighted)\n",
        "df_filtered['sentiment_label'] = df_filtered['sentiment_score'].apply(get_sentiment_label)\n",
        "\n",
        "# Filter out rows where the party is 'Unknown'\n",
        "df_final = df_filtered[df_filtered['party'] != 'Unknown']\n",
        "\n",
        "# Check the number of rows processed\n",
        "processed_rows = len(df_final)\n",
        "total_rows = len(df_filtered)\n",
        "print(f\"Sentiment analysis completed for {processed_rows} out of {total_rows} rows with identified parties.\")\n",
        "\n",
        "# Save filtered DataFrame to Excel\n",
        "file_path = '/content/output.xlsx'\n",
        "df_final.to_excel(file_path, sheet_name='Sheet1', index=False)\n",
        "\n",
        "if os.path.exists(file_path):\n",
        "    print(\"Excel file found. Attempting to download...\")\n",
        "    files.download(file_path)\n",
        "else:\n",
        "    print(\"Excel file not found.\")\n",
        "\n",
        "# Save filtered DataFrame to CSV\n",
        "csv_file_path = '/content/output_full_content.csv'\n",
        "df_final.to_csv(csv_file_path, index=False)\n",
        "\n",
        "# Verify the content of the saved CSV\n",
        "if os.path.exists(csv_file_path):\n",
        "    print(\"CSV file with full content found.\")\n",
        "    df_check = pd.read_csv(csv_file_path)\n",
        "    print(\"Preview of saved CSV:\")\n",
        "    print(df_check.head())  # Display the first few rows for verification\n",
        "    files.download(csv_file_path)\n",
        "else:\n",
        "    print(\"CSV file not found.\")\n",
        "\n",
        "# Save bad request URLs to Excel\n",
        "bad_requests_df = pd.DataFrame({'Bad URLs': bad_requests})\n",
        "bad_requests_file_path = '/content/badrequests.xlsx'\n",
        "bad_requests_df.to_excel(bad_requests_file_path, index=False)\n",
        "\n",
        "if os.path.exists(bad_requests_file_path):\n",
        "    print(\"Bad request URLs saved to 'badrequests.xlsx'\")\n",
        "else:\n",
        "    print(\"Failed to save bad request URLs.\")\n",
        "\n",
        "print(\"Script executed successfully\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LIO2MgTn6PiH"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}